{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8CCUOkTtf9U8ylvAebAGW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganesh3/pytorch-work/blob/master/pytorch_dataloader_dropout_lr_schedule.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset for this colab is available at http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks). Please download the sonar.all-data file and rename the type as cxv to upload to colab or in your local environment."
      ],
      "metadata": {
        "id": "oYGeu_kXXmNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have referred to these links which are from machine learning mastery for my learning.\n",
        "1. [link1](https://machinelearningmastery.com/training-a-pytorch-model-with-dataloader-and-dataset/)   \n",
        "2. [link2](https://machinelearningmastery.com/using-learning-rate-schedule-in-pytorch-training/) \n",
        "3. [link3](https://machinelearningmastery.com/using-dropout-regularization-in-pytorch-models/)\n",
        "\n",
        "Please use the same if you want to refer it."
      ],
      "metadata": {
        "id": "zOqiwz1BX46b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kEtWcKP7POvi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, default_collate\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data, convert to NumPy arrays\n",
        "df = pd.read_csv(\"sonar.csv\", header=None)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "HWPGPHoUX3g6",
        "outputId": "2d1d9452-af9e-43b7-9425-a1ae94eaae0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0       1       2       3       4       5       6       7       8   \\\n",
              "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
              "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
              "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
              "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
              "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
              "\n",
              "       9   ...      51      52      53      54      55      56      57  \\\n",
              "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
              "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
              "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
              "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
              "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
              "\n",
              "       58      59  60  \n",
              "0  0.0090  0.0032   R  \n",
              "1  0.0052  0.0044   R  \n",
              "2  0.0095  0.0078   R  \n",
              "3  0.0040  0.0117   R  \n",
              "4  0.0107  0.0094   R  \n",
              "\n",
              "[5 rows x 61 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1986b3e1-b0d2-41c1-bb17-addc88c81919\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 61 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1986b3e1-b0d2-41c1-bb17-addc88c81919')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1986b3e1-b0d2-41c1-bb17-addc88c81919 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1986b3e1-b0d2-41c1-bb17-addc88c81919');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYlbs21_YrfW",
        "outputId": "252fd94b-3b5b-4c1d-be5a-87b90e9608d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(208, 61)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 0:60].values\n",
        "y = df.iloc[:, 60].values"
      ],
      "metadata": {
        "id": "SL1oNd02Yx4J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "y = encoder.transform(y)"
      ],
      "metadata": {
        "id": "mP7FNF-XZAXr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conver to pytorch tensor\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "ixxCYWWuZg8h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)"
      ],
      "metadata": {
        "id": "hP6m8qPDbKIW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create DataLoader, then take one batch\n",
        "dls = DataLoader(list(zip(X_train, y_train)), shuffle=True, batch_size=16)"
      ],
      "metadata": {
        "id": "lFPet2BoZ63n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X_batch, y_batch in dls:\n",
        "  print(X_batch, y_batch)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrFc2IbPZ8PQ",
        "outputId": "cd79157c-0b01-4b34-f059-3468b186a3d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0408, 0.0653, 0.0397, 0.0604, 0.0496, 0.1817, 0.1178, 0.1024, 0.0583,\n",
            "         0.2176, 0.2459, 0.3332, 0.3087, 0.2613, 0.3232, 0.3731, 0.4203, 0.5364,\n",
            "         0.7062, 0.8196, 0.8835, 0.8299, 0.7609, 0.7605, 0.8367, 0.8905, 0.7652,\n",
            "         0.5897, 0.3037, 0.0823, 0.2787, 0.7241, 0.8032, 0.8050, 0.7676, 0.7468,\n",
            "         0.6253, 0.1730, 0.2916, 0.5003, 0.5220, 0.4824, 0.4004, 0.3877, 0.1651,\n",
            "         0.0442, 0.0663, 0.0418, 0.0475, 0.0235, 0.0066, 0.0062, 0.0129, 0.0184,\n",
            "         0.0069, 0.0198, 0.0199, 0.0102, 0.0070, 0.0055],\n",
            "        [0.0206, 0.0132, 0.0533, 0.0569, 0.0647, 0.1432, 0.1344, 0.2041, 0.1571,\n",
            "         0.1573, 0.2327, 0.1785, 0.1507, 0.1916, 0.2061, 0.2307, 0.2360, 0.1299,\n",
            "         0.3812, 0.5858, 0.4497, 0.4876, 1.0000, 0.8675, 0.4718, 0.5341, 0.6197,\n",
            "         0.7143, 0.5605, 0.3728, 0.2481, 0.1921, 0.1386, 0.3325, 0.2883, 0.3228,\n",
            "         0.2607, 0.2040, 0.2396, 0.1319, 0.0683, 0.0334, 0.0716, 0.0976, 0.0787,\n",
            "         0.0522, 0.0500, 0.0231, 0.0221, 0.0144, 0.0307, 0.0386, 0.0147, 0.0018,\n",
            "         0.0100, 0.0096, 0.0077, 0.0180, 0.0109, 0.0070],\n",
            "        [0.0228, 0.0853, 0.1000, 0.0428, 0.1117, 0.1651, 0.1597, 0.2116, 0.3295,\n",
            "         0.3517, 0.3330, 0.3643, 0.4020, 0.4731, 0.5196, 0.6573, 0.8426, 0.8476,\n",
            "         0.8344, 0.8453, 0.7999, 0.8537, 0.9642, 1.0000, 0.9357, 0.9409, 0.9070,\n",
            "         0.7104, 0.6320, 0.5667, 0.3501, 0.2447, 0.1698, 0.3290, 0.3674, 0.2331,\n",
            "         0.2413, 0.2556, 0.1892, 0.1940, 0.3074, 0.2785, 0.0308, 0.1238, 0.1854,\n",
            "         0.1753, 0.1079, 0.0728, 0.0242, 0.0191, 0.0159, 0.0172, 0.0191, 0.0260,\n",
            "         0.0140, 0.0125, 0.0116, 0.0093, 0.0012, 0.0036],\n",
            "        [0.0238, 0.0318, 0.0422, 0.0399, 0.0788, 0.0766, 0.0881, 0.1143, 0.1594,\n",
            "         0.2048, 0.2652, 0.3100, 0.2381, 0.1918, 0.1430, 0.1735, 0.1781, 0.2852,\n",
            "         0.5036, 0.6166, 0.7616, 0.8125, 0.7793, 0.8788, 0.8813, 0.9470, 1.0000,\n",
            "         0.9739, 0.8446, 0.6151, 0.4302, 0.3165, 0.2869, 0.2017, 0.1206, 0.0271,\n",
            "         0.0580, 0.1262, 0.1072, 0.1082, 0.0360, 0.1197, 0.2061, 0.2054, 0.1878,\n",
            "         0.2047, 0.1716, 0.1069, 0.0477, 0.0170, 0.0186, 0.0096, 0.0071, 0.0084,\n",
            "         0.0038, 0.0026, 0.0028, 0.0013, 0.0035, 0.0060],\n",
            "        [0.0192, 0.0607, 0.0378, 0.0774, 0.1388, 0.0809, 0.0568, 0.0219, 0.1037,\n",
            "         0.1186, 0.1237, 0.1601, 0.3520, 0.4479, 0.3769, 0.5761, 0.6426, 0.6790,\n",
            "         0.7157, 0.5466, 0.5399, 0.6362, 0.7849, 0.7756, 0.5780, 0.4862, 0.4181,\n",
            "         0.2457, 0.0716, 0.0613, 0.1816, 0.4493, 0.5976, 0.3785, 0.2495, 0.5771,\n",
            "         0.8852, 0.8409, 0.3570, 0.3133, 0.6096, 0.6378, 0.2709, 0.1419, 0.1260,\n",
            "         0.1288, 0.0790, 0.0829, 0.0520, 0.0216, 0.0360, 0.0331, 0.0131, 0.0120,\n",
            "         0.0108, 0.0024, 0.0045, 0.0037, 0.0112, 0.0075],\n",
            "        [0.1021, 0.0830, 0.0577, 0.0627, 0.0635, 0.1328, 0.0988, 0.1787, 0.1199,\n",
            "         0.1369, 0.2509, 0.2631, 0.2796, 0.2977, 0.3823, 0.3129, 0.3956, 0.2093,\n",
            "         0.3218, 0.3345, 0.3184, 0.2887, 0.3610, 0.2566, 0.4106, 0.4591, 0.4722,\n",
            "         0.7278, 0.7591, 0.6579, 0.7514, 0.6666, 0.4903, 0.5962, 0.6552, 0.4014,\n",
            "         0.1188, 0.3245, 0.3107, 0.1354, 0.5109, 0.7988, 0.7517, 0.5508, 0.5858,\n",
            "         0.7292, 0.5522, 0.3339, 0.1608, 0.0475, 0.1004, 0.0709, 0.0317, 0.0309,\n",
            "         0.0252, 0.0087, 0.0177, 0.0214, 0.0227, 0.0106],\n",
            "        [0.0530, 0.0885, 0.1997, 0.2604, 0.3225, 0.2247, 0.0617, 0.2287, 0.0950,\n",
            "         0.0740, 0.1610, 0.2226, 0.2703, 0.3365, 0.4266, 0.4144, 0.5655, 0.6921,\n",
            "         0.8547, 0.9234, 0.9171, 1.0000, 0.9532, 0.9101, 0.8337, 0.7053, 0.6534,\n",
            "         0.4483, 0.2460, 0.2020, 0.1446, 0.0994, 0.1510, 0.2392, 0.4434, 0.5023,\n",
            "         0.4441, 0.4571, 0.3927, 0.2900, 0.3408, 0.4990, 0.3632, 0.1387, 0.1800,\n",
            "         0.1299, 0.0523, 0.0817, 0.0469, 0.0114, 0.0299, 0.0244, 0.0199, 0.0257,\n",
            "         0.0082, 0.0151, 0.0171, 0.0146, 0.0134, 0.0056],\n",
            "        [0.0519, 0.0548, 0.0842, 0.0319, 0.1158, 0.0922, 0.1027, 0.0613, 0.1465,\n",
            "         0.2838, 0.2802, 0.3086, 0.2657, 0.3801, 0.5626, 0.4376, 0.2617, 0.1199,\n",
            "         0.6676, 0.9402, 0.7832, 0.5352, 0.6809, 0.9174, 0.7613, 0.8220, 0.8872,\n",
            "         0.6091, 0.2967, 0.1103, 0.1318, 0.0624, 0.0990, 0.4006, 0.3666, 0.1050,\n",
            "         0.1915, 0.3930, 0.4288, 0.2546, 0.1151, 0.2196, 0.1879, 0.1437, 0.2146,\n",
            "         0.2360, 0.1125, 0.0254, 0.0285, 0.0178, 0.0052, 0.0081, 0.0120, 0.0045,\n",
            "         0.0121, 0.0097, 0.0085, 0.0047, 0.0048, 0.0053],\n",
            "        [0.0089, 0.0274, 0.0248, 0.0237, 0.0224, 0.0845, 0.1488, 0.1224, 0.1569,\n",
            "         0.2119, 0.3003, 0.3094, 0.2743, 0.2547, 0.1870, 0.1452, 0.1457, 0.2429,\n",
            "         0.3259, 0.3679, 0.3355, 0.3100, 0.3914, 0.5280, 0.6409, 0.7707, 0.8754,\n",
            "         1.0000, 0.9806, 0.6969, 0.4973, 0.5020, 0.5359, 0.3842, 0.1848, 0.1149,\n",
            "         0.1570, 0.1311, 0.1583, 0.2631, 0.3103, 0.4512, 0.3785, 0.1269, 0.1459,\n",
            "         0.1092, 0.1485, 0.1385, 0.0716, 0.0176, 0.0199, 0.0096, 0.0103, 0.0093,\n",
            "         0.0025, 0.0044, 0.0021, 0.0069, 0.0060, 0.0018],\n",
            "        [0.0068, 0.0232, 0.0513, 0.0444, 0.0249, 0.0637, 0.0422, 0.1130, 0.1911,\n",
            "         0.2475, 0.1606, 0.0922, 0.2398, 0.3220, 0.4295, 0.2652, 0.0666, 0.1442,\n",
            "         0.2373, 0.2595, 0.2493, 0.3903, 0.6384, 0.8037, 0.7026, 0.6874, 0.6997,\n",
            "         0.8558, 1.0000, 0.9621, 0.8996, 0.7575, 0.6902, 0.5686, 0.4396, 0.4546,\n",
            "         0.2959, 0.1587, 0.1681, 0.0842, 0.1173, 0.1754, 0.2728, 0.1705, 0.0194,\n",
            "         0.0213, 0.0354, 0.0420, 0.0093, 0.0204, 0.0199, 0.0173, 0.0163, 0.0055,\n",
            "         0.0045, 0.0068, 0.0041, 0.0052, 0.0194, 0.0105],\n",
            "        [0.0394, 0.0420, 0.0446, 0.0551, 0.0597, 0.1416, 0.0956, 0.0802, 0.1618,\n",
            "         0.2558, 0.3078, 0.3404, 0.3400, 0.3951, 0.3352, 0.2252, 0.2086, 0.2248,\n",
            "         0.3382, 0.4578, 0.6474, 0.6708, 0.7007, 0.7619, 0.7745, 0.6767, 0.7373,\n",
            "         0.7834, 0.9619, 1.0000, 0.8086, 0.5558, 0.5409, 0.4988, 0.3108, 0.2897,\n",
            "         0.2244, 0.0960, 0.2287, 0.3228, 0.3454, 0.3882, 0.3240, 0.0926, 0.1173,\n",
            "         0.0566, 0.0766, 0.0969, 0.0588, 0.0050, 0.0118, 0.0146, 0.0040, 0.0114,\n",
            "         0.0032, 0.0062, 0.0101, 0.0068, 0.0053, 0.0087],\n",
            "        [0.0239, 0.0189, 0.0466, 0.0440, 0.0657, 0.0742, 0.1380, 0.1099, 0.1384,\n",
            "         0.1376, 0.0938, 0.0259, 0.1499, 0.2851, 0.5743, 0.8278, 0.8669, 0.8131,\n",
            "         0.9045, 0.9046, 1.0000, 0.9976, 0.9872, 0.9761, 0.9009, 0.9724, 0.9675,\n",
            "         0.7633, 0.4434, 0.3822, 0.4727, 0.4007, 0.3381, 0.3172, 0.2222, 0.0733,\n",
            "         0.2692, 0.1888, 0.0712, 0.1062, 0.0694, 0.0300, 0.0893, 0.1459, 0.1348,\n",
            "         0.0391, 0.0546, 0.0469, 0.0201, 0.0095, 0.0155, 0.0091, 0.0151, 0.0080,\n",
            "         0.0018, 0.0078, 0.0045, 0.0026, 0.0036, 0.0024],\n",
            "        [0.0731, 0.1249, 0.1665, 0.1496, 0.1443, 0.2770, 0.2555, 0.1712, 0.0466,\n",
            "         0.1114, 0.1739, 0.3160, 0.3249, 0.2164, 0.2031, 0.2580, 0.1796, 0.2422,\n",
            "         0.3609, 0.1810, 0.2604, 0.6572, 0.9734, 0.9757, 0.8079, 0.6521, 0.4915,\n",
            "         0.5363, 0.7649, 0.5250, 0.5101, 0.4219, 0.4160, 0.1906, 0.0223, 0.4219,\n",
            "         0.5496, 0.2483, 0.2034, 0.2729, 0.2837, 0.4463, 0.3178, 0.0807, 0.1192,\n",
            "         0.2134, 0.3241, 0.2945, 0.1474, 0.0211, 0.0361, 0.0444, 0.0230, 0.0290,\n",
            "         0.0141, 0.0161, 0.0177, 0.0194, 0.0207, 0.0057],\n",
            "        [0.0095, 0.0308, 0.0539, 0.0411, 0.0613, 0.1039, 0.1016, 0.1394, 0.2592,\n",
            "         0.3745, 0.4229, 0.4499, 0.5404, 0.4303, 0.3333, 0.3496, 0.3426, 0.2851,\n",
            "         0.4062, 0.6833, 0.7650, 0.6670, 0.5703, 0.5995, 0.6484, 0.8614, 0.9819,\n",
            "         0.9380, 0.8435, 0.6074, 0.5403, 0.6890, 0.5977, 0.3244, 0.0516, 0.3157,\n",
            "         0.3590, 0.3881, 0.5716, 0.4314, 0.3051, 0.4393, 0.4302, 0.4831, 0.5084,\n",
            "         0.1952, 0.1539, 0.2037, 0.1054, 0.0251, 0.0357, 0.0181, 0.0019, 0.0102,\n",
            "         0.0133, 0.0040, 0.0042, 0.0030, 0.0031, 0.0033],\n",
            "        [0.0270, 0.0092, 0.0145, 0.0278, 0.0412, 0.0757, 0.1026, 0.1138, 0.0794,\n",
            "         0.1520, 0.1675, 0.1370, 0.1361, 0.1345, 0.2144, 0.5354, 0.6830, 0.5600,\n",
            "         0.3093, 0.3226, 0.4430, 0.5573, 0.5782, 0.6173, 0.8132, 0.9819, 0.9823,\n",
            "         0.9166, 0.7423, 0.7736, 0.8473, 0.7352, 0.6671, 0.6083, 0.6239, 0.5972,\n",
            "         0.5715, 0.5242, 0.2924, 0.1536, 0.2003, 0.2031, 0.2207, 0.1778, 0.1353,\n",
            "         0.1373, 0.0749, 0.0472, 0.0325, 0.0179, 0.0045, 0.0084, 0.0010, 0.0018,\n",
            "         0.0068, 0.0039, 0.0120, 0.0132, 0.0070, 0.0088],\n",
            "        [0.0308, 0.0339, 0.0202, 0.0889, 0.1570, 0.1750, 0.0920, 0.1353, 0.1593,\n",
            "         0.2795, 0.3336, 0.2940, 0.1608, 0.3335, 0.4985, 0.7295, 0.7350, 0.8253,\n",
            "         0.8793, 0.9657, 1.0000, 0.8707, 0.6471, 0.5973, 0.8218, 0.7755, 0.6111,\n",
            "         0.4195, 0.2990, 0.1354, 0.2438, 0.5624, 0.5555, 0.6963, 0.7298, 0.7022,\n",
            "         0.5468, 0.1421, 0.4738, 0.6410, 0.4375, 0.3178, 0.2377, 0.2808, 0.1374,\n",
            "         0.1136, 0.1034, 0.0688, 0.0422, 0.0117, 0.0070, 0.0167, 0.0127, 0.0138,\n",
            "         0.0090, 0.0051, 0.0029, 0.0122, 0.0056, 0.0020]]) tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(60, 60),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(60, 30),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(30, 1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "wjsMd-7Ladg-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define loss function & optimize\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "thRTqhEIceD-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 200\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  for X_batch, y_batch in dls:\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()  "
      ],
      "metadata": {
        "id": "pSE2T36EcLTt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate accuracy after training\n",
        "model.eval()\n",
        "y_test_pred = model(X_test)\n",
        "acc = (y_test_pred.round() == y_test).float().mean()\n",
        "acc = float(acc)\n",
        "print(\"Model accuracy: %.2f%%\" % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBWVjB01gzbB",
        "outputId": "426bc4dc-8bdc-4ede-e940-a9d3d8bf2f9a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 80.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LKdSTVnjYQ5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Data Iterator using Dataset Class"
      ],
      "metadata": {
        "id": "0KpfBPOrjZxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SonarDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    \"\"\" convert into PyTorch tensors and remember them\"\"\"\n",
        "    self.X = torch.tensor(X, dtype=torch.float32)\n",
        "    self.y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\" this should return the size of the dataset \"\"\"\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\" this should return one sample from the dataset \"\"\"\n",
        "    features = self.X[idx]\n",
        "    target = self.y[idx]\n",
        "    return features, target"
      ],
      "metadata": {
        "id": "wiolA_dihTRh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up DataLoader for training set\n",
        "dataset = SonarDataset(X_train, y_train)\n",
        "loader = DataLoader(dataset, shuffle=True, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1Jor7_VkqiC",
        "outputId": "4f147353-a6ad-4d56-fb6a-1c60b2e31ea0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-5f16543aaeb8>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.X = torch.tensor(X, dtype=torch.float32)\n",
            "<ipython-input-14-5f16543aaeb8>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 200\n",
        "model.train()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  for X_batch, y_batch in dls:\n",
        "    y_pred = model(X_batch)\n",
        "    loss = loss_fn(y_pred, y_batch)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "qWSL7HgNkuFY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_test_pred = model(X_test)\n",
        "acc = (y_test_pred.round() == y_test).float().mean()\n",
        "acc = float(acc)\n",
        "print(\"Model accuracy: %.2f%%\" % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp4bTc2i0DZH",
        "outputId": "b6f027ef-2504-4f41-8d68-fca1178e1f53"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 50.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up DataLoader for data set\n",
        "trainset, testset = random_split(dataset, [0.7, 0.3])\n",
        "loader = DataLoader(trainset, shuffle=True, batch_size=16)"
      ],
      "metadata": {
        "id": "lmmEt82o0EAy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of how default_collate works\n",
        "default_collate([[0, 1], [2, 4]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBnVhLOJ3jFR",
        "outputId": "ccf4eb8c-569b-4466-b25d-382d0bd33c89"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([0, 2]), tensor([1, 4])]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "n_epochs = 200\n",
        "loss_fn = nn.BCELoss()\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    for X_batch, y_batch in loader:\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# create one test tensor from the testset\n",
        "#  use the default_collate() function to collect samples from a dataset into tensors\n",
        "X_test, y_test = default_collate(testset)\n",
        "model.eval()\n",
        "y_pred = model(X_test)\n",
        "acc = (y_pred.round() == y_test).float().mean()\n",
        "acc = float(acc)\n",
        "print(\"Model accuracy: %.2f%%\" % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-irc5C10MGf",
        "outputId": "b173c705-972b-46ac-90bf-5d2d0b491a81"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 90.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tpl = (10, 20, 30, 40)\n",
        "print(tpl[::-1][list(enumerate(tpl, start=1))[-1][0]-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RztmtHuo3p_C",
        "outputId": "966c0455-de52-4ee8-fbe1-264ee5cbd3be"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying Learning Rate Schedules in PyTorch Training"
      ],
      "metadata": {
        "id": "y7PGrcSGJ-Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)"
      ],
      "metadata": {
        "id": "6tHk2xFp7PI1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "n_epochs = 50\n",
        "batch_size = 24\n",
        "batch_start = torch.arange(0, len(X_train), batch_size)\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    for start in batch_start:\n",
        "        X_batch = X_train[start:start+batch_size]\n",
        "        y_batch = y_train[start:start+batch_size]\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    #print(optim.param_groups[0])\n",
        "    scheduler.step()\n",
        "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n",
        " \n",
        "# evaluate accuracy after training\n",
        "model.eval()\n",
        "y_pred = model(X_test)\n",
        "acc = (y_pred.round() == y_test).float().mean()\n",
        "acc = float(acc)\n",
        "print(\"Model accuracy: %.2f%%\" % (acc*100))"
      ],
      "metadata": {
        "id": "3jzBEaV67ZiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e360a99-eaf4-44ab-982f-680180e4cede"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: SGD lr 0.1000 -> 0.0983\n",
            "Epoch 1: SGD lr 0.0983 -> 0.0967\n",
            "Epoch 2: SGD lr 0.0967 -> 0.0950\n",
            "Epoch 3: SGD lr 0.0950 -> 0.0933\n",
            "Epoch 4: SGD lr 0.0933 -> 0.0917\n",
            "Epoch 5: SGD lr 0.0917 -> 0.0900\n",
            "Epoch 6: SGD lr 0.0900 -> 0.0883\n",
            "Epoch 7: SGD lr 0.0883 -> 0.0867\n",
            "Epoch 8: SGD lr 0.0867 -> 0.0850\n",
            "Epoch 9: SGD lr 0.0850 -> 0.0833\n",
            "Epoch 10: SGD lr 0.0833 -> 0.0817\n",
            "Epoch 11: SGD lr 0.0817 -> 0.0800\n",
            "Epoch 12: SGD lr 0.0800 -> 0.0783\n",
            "Epoch 13: SGD lr 0.0783 -> 0.0767\n",
            "Epoch 14: SGD lr 0.0767 -> 0.0750\n",
            "Epoch 15: SGD lr 0.0750 -> 0.0733\n",
            "Epoch 16: SGD lr 0.0733 -> 0.0717\n",
            "Epoch 17: SGD lr 0.0717 -> 0.0700\n",
            "Epoch 18: SGD lr 0.0700 -> 0.0683\n",
            "Epoch 19: SGD lr 0.0683 -> 0.0667\n",
            "Epoch 20: SGD lr 0.0667 -> 0.0650\n",
            "Epoch 21: SGD lr 0.0650 -> 0.0633\n",
            "Epoch 22: SGD lr 0.0633 -> 0.0617\n",
            "Epoch 23: SGD lr 0.0617 -> 0.0600\n",
            "Epoch 24: SGD lr 0.0600 -> 0.0583\n",
            "Epoch 25: SGD lr 0.0583 -> 0.0567\n",
            "Epoch 26: SGD lr 0.0567 -> 0.0550\n",
            "Epoch 27: SGD lr 0.0550 -> 0.0533\n",
            "Epoch 28: SGD lr 0.0533 -> 0.0517\n",
            "Epoch 29: SGD lr 0.0517 -> 0.0500\n",
            "Epoch 30: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 31: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 32: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 33: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 34: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 35: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 36: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 37: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 38: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 39: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 40: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 41: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 42: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 43: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 44: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 45: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 46: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 47: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 48: SGD lr 0.0500 -> 0.0500\n",
            "Epoch 49: SGD lr 0.0500 -> 0.0500\n",
            "Model accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.99)"
      ],
      "metadata": {
        "id": "Hvgh1PNKL740"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "n_epoch = 50\n",
        "batch_size = 24\n",
        "batch_start = torch.arange(0, len(X_train), batch_size)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  for start in batch_start:\n",
        "    X_batch = X_train[start:start+batch_size]\n",
        "    y_batch = y_train[start:start+batch_size]\n",
        "    y_pred = model(X_batch)\n",
        "    loss = loss_fn(y_pred, y_batch)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  before_lr = optimizer.param_groups[0][\"lr\"]\n",
        "  scheduler.step()\n",
        "  after_lr = optimizer.param_groups[0][\"lr\"]\n",
        "  print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE1vMeUiOFqf",
        "outputId": "636b4d99-0903-4565-9925-e15c136ca827"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: SGD lr 0.0500 -> 0.0495\n",
            "Epoch 1: SGD lr 0.0495 -> 0.0490\n",
            "Epoch 2: SGD lr 0.0490 -> 0.0485\n",
            "Epoch 3: SGD lr 0.0485 -> 0.0480\n",
            "Epoch 4: SGD lr 0.0480 -> 0.0475\n",
            "Epoch 5: SGD lr 0.0475 -> 0.0471\n",
            "Epoch 6: SGD lr 0.0471 -> 0.0466\n",
            "Epoch 7: SGD lr 0.0466 -> 0.0461\n",
            "Epoch 8: SGD lr 0.0461 -> 0.0457\n",
            "Epoch 9: SGD lr 0.0457 -> 0.0452\n",
            "Epoch 10: SGD lr 0.0452 -> 0.0448\n",
            "Epoch 11: SGD lr 0.0448 -> 0.0443\n",
            "Epoch 12: SGD lr 0.0443 -> 0.0439\n",
            "Epoch 13: SGD lr 0.0439 -> 0.0434\n",
            "Epoch 14: SGD lr 0.0434 -> 0.0430\n",
            "Epoch 15: SGD lr 0.0430 -> 0.0426\n",
            "Epoch 16: SGD lr 0.0426 -> 0.0421\n",
            "Epoch 17: SGD lr 0.0421 -> 0.0417\n",
            "Epoch 18: SGD lr 0.0417 -> 0.0413\n",
            "Epoch 19: SGD lr 0.0413 -> 0.0409\n",
            "Epoch 20: SGD lr 0.0409 -> 0.0405\n",
            "Epoch 21: SGD lr 0.0405 -> 0.0401\n",
            "Epoch 22: SGD lr 0.0401 -> 0.0397\n",
            "Epoch 23: SGD lr 0.0397 -> 0.0393\n",
            "Epoch 24: SGD lr 0.0393 -> 0.0389\n",
            "Epoch 25: SGD lr 0.0389 -> 0.0385\n",
            "Epoch 26: SGD lr 0.0385 -> 0.0381\n",
            "Epoch 27: SGD lr 0.0381 -> 0.0377\n",
            "Epoch 28: SGD lr 0.0377 -> 0.0374\n",
            "Epoch 29: SGD lr 0.0374 -> 0.0370\n",
            "Epoch 30: SGD lr 0.0370 -> 0.0366\n",
            "Epoch 31: SGD lr 0.0366 -> 0.0362\n",
            "Epoch 32: SGD lr 0.0362 -> 0.0359\n",
            "Epoch 33: SGD lr 0.0359 -> 0.0355\n",
            "Epoch 34: SGD lr 0.0355 -> 0.0352\n",
            "Epoch 35: SGD lr 0.0352 -> 0.0348\n",
            "Epoch 36: SGD lr 0.0348 -> 0.0345\n",
            "Epoch 37: SGD lr 0.0345 -> 0.0341\n",
            "Epoch 38: SGD lr 0.0341 -> 0.0338\n",
            "Epoch 39: SGD lr 0.0338 -> 0.0334\n",
            "Epoch 40: SGD lr 0.0334 -> 0.0331\n",
            "Epoch 41: SGD lr 0.0331 -> 0.0328\n",
            "Epoch 42: SGD lr 0.0328 -> 0.0325\n",
            "Epoch 43: SGD lr 0.0325 -> 0.0321\n",
            "Epoch 44: SGD lr 0.0321 -> 0.0318\n",
            "Epoch 45: SGD lr 0.0318 -> 0.0315\n",
            "Epoch 46: SGD lr 0.0315 -> 0.0312\n",
            "Epoch 47: SGD lr 0.0312 -> 0.0309\n",
            "Epoch 48: SGD lr 0.0309 -> 0.0306\n",
            "Epoch 49: SGD lr 0.0306 -> 0.0303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate accuracy after training\n",
        "model.eval()\n",
        "y_pred = model(X_test)\n",
        "acc = (y_pred.round() == y_test).float().mean()\n",
        "acc = float(acc)\n",
        "print(\"Model accuracy: %.2f%%\" % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsCB1s9QPaqa",
        "outputId": "8d83123e-0ca7-4c13-a1fe-74e8fa6983ba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Learning Rate Schedules"
      ],
      "metadata": {
        "id": "bAByuLuDPq_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A custom learning rate schedule can be defined using a custom function. For example, you want to have a learning rate that:\n",
        "\n",
        "$l r_n=\\frac{l r_0}{1+\\alpha n}$"
      ],
      "metadata": {
        "id": "I3r3DFgevLuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_lambda(epoch):\n",
        "  # LR to be 0.1 * (1/1+0.01*epoch)\n",
        "  base_lr = 0.1\n",
        "  factor = 0.01\n",
        "  return base_lr/(1+ factor * epoch)"
      ],
      "metadata": {
        "id": "fZ8ZouPAPnbg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)"
      ],
      "metadata": {
        "id": "8eF49wqLwPzm"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "n_epochs = 50\n",
        "batch_size = 24\n",
        "batch_start = torch.arange(0, len(X_train), batch_size)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  for size in batch_start:\n",
        "    X_batch = X_train[start: start+batch_size]\n",
        "    y_batch = y_train[start: start+batch_size]\n",
        "\n",
        "    y_pred = model(X_batch)\n",
        "    loss = loss_fn(y_pred, y_batch)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  before_lr = optimizer.param_groups[0][\"lr\"]\n",
        "  scheduler.step()\n",
        "  after_lr = optimizer.param_groups[0][\"lr\"]\n",
        "  print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPE-bHi9wij0",
        "outputId": "8807639b-fa3d-4ded-a040-6ade8eec19b6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: SGD lr 0.0100 -> 0.0099\n",
            "Epoch 1: SGD lr 0.0099 -> 0.0098\n",
            "Epoch 2: SGD lr 0.0098 -> 0.0097\n",
            "Epoch 3: SGD lr 0.0097 -> 0.0096\n",
            "Epoch 4: SGD lr 0.0096 -> 0.0095\n",
            "Epoch 5: SGD lr 0.0095 -> 0.0094\n",
            "Epoch 6: SGD lr 0.0094 -> 0.0093\n",
            "Epoch 7: SGD lr 0.0093 -> 0.0093\n",
            "Epoch 8: SGD lr 0.0093 -> 0.0092\n",
            "Epoch 9: SGD lr 0.0092 -> 0.0091\n",
            "Epoch 10: SGD lr 0.0091 -> 0.0090\n",
            "Epoch 11: SGD lr 0.0090 -> 0.0089\n",
            "Epoch 12: SGD lr 0.0089 -> 0.0088\n",
            "Epoch 13: SGD lr 0.0088 -> 0.0088\n",
            "Epoch 14: SGD lr 0.0088 -> 0.0087\n",
            "Epoch 15: SGD lr 0.0087 -> 0.0086\n",
            "Epoch 16: SGD lr 0.0086 -> 0.0085\n",
            "Epoch 17: SGD lr 0.0085 -> 0.0085\n",
            "Epoch 18: SGD lr 0.0085 -> 0.0084\n",
            "Epoch 19: SGD lr 0.0084 -> 0.0083\n",
            "Epoch 20: SGD lr 0.0083 -> 0.0083\n",
            "Epoch 21: SGD lr 0.0083 -> 0.0082\n",
            "Epoch 22: SGD lr 0.0082 -> 0.0081\n",
            "Epoch 23: SGD lr 0.0081 -> 0.0081\n",
            "Epoch 24: SGD lr 0.0081 -> 0.0080\n",
            "Epoch 25: SGD lr 0.0080 -> 0.0079\n",
            "Epoch 26: SGD lr 0.0079 -> 0.0079\n",
            "Epoch 27: SGD lr 0.0079 -> 0.0078\n",
            "Epoch 28: SGD lr 0.0078 -> 0.0078\n",
            "Epoch 29: SGD lr 0.0078 -> 0.0077\n",
            "Epoch 30: SGD lr 0.0077 -> 0.0076\n",
            "Epoch 31: SGD lr 0.0076 -> 0.0076\n",
            "Epoch 32: SGD lr 0.0076 -> 0.0075\n",
            "Epoch 33: SGD lr 0.0075 -> 0.0075\n",
            "Epoch 34: SGD lr 0.0075 -> 0.0074\n",
            "Epoch 35: SGD lr 0.0074 -> 0.0074\n",
            "Epoch 36: SGD lr 0.0074 -> 0.0073\n",
            "Epoch 37: SGD lr 0.0073 -> 0.0072\n",
            "Epoch 38: SGD lr 0.0072 -> 0.0072\n",
            "Epoch 39: SGD lr 0.0072 -> 0.0071\n",
            "Epoch 40: SGD lr 0.0071 -> 0.0071\n",
            "Epoch 41: SGD lr 0.0071 -> 0.0070\n",
            "Epoch 42: SGD lr 0.0070 -> 0.0070\n",
            "Epoch 43: SGD lr 0.0070 -> 0.0069\n",
            "Epoch 44: SGD lr 0.0069 -> 0.0069\n",
            "Epoch 45: SGD lr 0.0069 -> 0.0068\n",
            "Epoch 46: SGD lr 0.0068 -> 0.0068\n",
            "Epoch 47: SGD lr 0.0068 -> 0.0068\n",
            "Epoch 48: SGD lr 0.0068 -> 0.0067\n",
            "Epoch 49: SGD lr 0.0067 -> 0.0067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# showing how batch_start looks like\n",
        "batch_start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nfh6hapwm5a",
        "outputId": "c671c716-196b-4cbe-ebec-8ff3330db048"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  0,  24,  48,  72,  96, 120, 144])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate accuracy after training\n",
        "model.eval()\n",
        "y_test_pred = model(X_test)\n",
        "acc = (y_test_pred.round() == y_test).float().mean()\n",
        "acc = float(acc)\n",
        "print(\"Model accuracy: %.2f%%\" % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz0orQrpxBkE",
        "outputId": "eee8af5d-d19f-4e66-e1f4-5cdc12fa247e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Dropout Regularization in PyTorch Models"
      ],
      "metadata": {
        "id": "MqkB3YqMzJeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout is a regularization technique for neural network models proposed around 2012 to 2014. It is a layer in the neural network. During training of a neural network model, it will take the output from its previous layer, randomly select some of the neurons and zero them out before passing to the next layer, effectively ignored them. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass, and any weight updates are not applied to the neuron on the backward pass.\n",
        "\n",
        "When the model is used for inference, dropout layer is just to scale all the neurons constantly to compensate the effect of dropping out during training."
      ],
      "metadata": {
        "id": "KYXs7M9G2tFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You do not need to randomly select elements from a PyTorch tensor to implement dropout manually. The nn.Dropout() layer from PyTorch can be introduced into your model. It is implemented by randomly selecting nodes to be dropped out with a given probability $p$ (e.g., 20%) while in the training loop. In PyTorch, the dropout layer further scale the resulting tensor by a factor of $\\frac{1}{1-p}$\n",
        "so the average tensor value is maintained. Thanks to this scaling, the dropout layer operates at inference will be an identify function (i.e., no effect, simply copy over the input tensor as output tensor). You should make sure to turn the model into inference mode when evaluating the the model"
      ],
      "metadata": {
        "id": "CnRor7pV5hes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define PyTorch model, with dropout at hidden layers\n",
        "class SonarModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(60, 60)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.05)\n",
        "        self.layer2 = nn.Linear(60, 30)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.output = nn.Linear(30, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.layer1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.act2(self.layer2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.sigmoid(self.output(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "Mq2qi3ZyyvE0"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to train the model and return the validation result\n",
        "def model_train(model, X_train, y_train, X_val, y_val,\n",
        "                n_epochs=300, batch_size=16):\n",
        "    loss_fn = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n",
        "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
        "    #scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
        "    #scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        for start in batch_start:\n",
        "            X_batch = X_train[start:start+batch_size]\n",
        "            y_batch = y_train[start:start+batch_size]\n",
        "            y_pred = model(X_batch)\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        #scheduler.step()\n",
        " \n",
        "    # evaluate accuracy after training\n",
        "    model.eval()\n",
        "    y_pred = model(X_val)\n",
        "    acc = (y_pred.round() == y_val).float().mean()\n",
        "    acc = float(acc)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "-r7PdgTp7rK_"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 0:60]\n",
        "y = df.iloc[:, 60]\n",
        " \n",
        "# Label encode the target from string to integer\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y)\n",
        "y = encoder.transform(y)\n",
        "\n",
        "# Convert to 2D PyTorch tensors\n",
        "X = torch.tensor(X.values, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
        "# run 10-fold cross validation\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "accuracies = []\n",
        "for train, test in kfold.split(X, y):\n",
        "  # create model, train, and get accuracy\n",
        "  model = SonarModel()\n",
        "  acc = model_train(model, X[train], y[train], X[test], y[test])\n",
        "  print(\"Accuracy: %.2f\" % acc)\n",
        "  accuracies.append(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJTaLudn98Vp",
        "outputId": "9f82c769-931d-474e-edd3-edc8f2e0900b"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.81\n",
            "Accuracy: 0.81\n",
            "Accuracy: 0.81\n",
            "Accuracy: 0.86\n",
            "Accuracy: 0.81\n",
            "Accuracy: 0.90\n",
            "Accuracy: 0.86\n",
            "Accuracy: 0.76\n",
            "Accuracy: 0.85\n",
            "Accuracy: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "mean = np.mean(accuracies)\n",
        "std = np.std(accuracies)\n",
        "print(\"Baseline: %.2f%% (+/- %.2f%%)\" % (mean*100, std*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuharznT-iTf",
        "outputId": "9e67444d-ed3c-4c3b-fce7-dbb311a6624b"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline: 83.19% (+/- 3.76%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "itnXdNH5_hx9"
      },
      "execution_count": 111,
      "outputs": []
    }
  ]
}